{
 "metadata": {
  "name": "",
  "signature": "sha256:de200c32dacd8c423f388a2149888432fca5783888d06e0b27be71b651b6be1e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# A Convolutional Neural Network for Classifying Laughter Clips"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "This script contains code for a Convolutional Neural Network\n",
      "that classifies a 10-second audio clip into one of five \n",
      "laughter categories: baby laughter, belly laugh, chuckle/chortle, \n",
      "giggle, snicker.\n",
      "\n",
      "Author: Ganesh Srinivas <gs401 [at] snu.edu.in>\n",
      "\"\"\"\n",
      "\n",
      "import glob\n",
      "import os\n",
      "import random\n",
      "\n",
      "import tensorflow as tf\n",
      "import librosa\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "plt.ion()\n",
      "\n",
      "## Dataset location\n",
      "FILENAMES = \"../dataset/audioset_laughter_clips/10secondclipfiles.txt\"\n",
      "DATASET_LOCATION = \"../dataset/audioset_laughter_clips/\"\n",
      "\n",
      "## Hyperparameters\n",
      "# for Learning algorithm\n",
      "learning_rate = 0.01\n",
      "batch_size = 225\n",
      "training_iterations = 10\n",
      "\n",
      "# for Feature extraction\n",
      "max_audio_length = 221184\n",
      "frames = 433\n",
      "bands = 60\n",
      "feature_size = frames*bands #433x60\n",
      "\n",
      "# for Network\n",
      "num_labels = 5\n",
      "num_channels = 2 \n",
      "kernel_size = 30\n",
      "depth = 20\n",
      "num_hidden = 200\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Helper functions for loading data and extracting features\n",
      "def labeltext2labelid(category_name):\n",
      "    \"\"\"\n",
      "    Returns a numerical label for each laughter category\n",
      "    \"\"\"\n",
      "    possible_categories = ['baby_laughter', 'belly_laugh', \\\n",
      "    'chuckle_chortle', 'giggle', 'snicker']\n",
      "    return possible_categories.index(category_name)\n",
      "\n",
      "def shape_sound_clip(sound_clip, required_length=max_audio_length):\n",
      "    \"\"\"\n",
      "    Shapes sound clips to have constant length\n",
      "    \"\"\"\n",
      "    z=np.zeros((required_length-sound_clip.shape[0],))\n",
      "    return np.append(sound_clip,z)\n",
      "\n",
      "def extract_features(filenames):\n",
      "    \"\"\"\n",
      "    Extract log-scaled mel-spectrograms and their corresponding \n",
      "    deltas from the sound clips\n",
      "    \"\"\"\n",
      "    log_specgrams = []\n",
      "    labels=[]\n",
      "    for f in filenames:\n",
      "      signal,s = librosa.load(f)\n",
      "      sound_clip = shape_sound_clip(signal)\n",
      "      melspec = librosa.feature.melspectrogram(sound_clip, n_mels = 60)\n",
      "      #print melspec.shape\n",
      "      logspec = librosa.logamplitude(melspec)\n",
      "      #print logspec.shape\n",
      "      logspec = logspec.T.flatten()[:, np.newaxis].T\n",
      "      #print logspec.shape\n",
      "      #print \"Produce of two elements in melspec: \", melspec.shape[0]*melspec.shape[1]  \n",
      "      log_specgrams.append(logspec)\n",
      "      labels.append(labeltext2labelid(f.split('/')[-2]))  \n",
      "    log_specgrams=np.asarray(log_specgrams).reshape(len(log_specgrams),60,433,1)\n",
      "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis=3)\n",
      "    for i in range(len(features)):\n",
      "          features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
      "    return np.array(features), np.array(labels,dtype=np.int)\n",
      "\n",
      "def one_hot_encode(labels, num_labels=num_labels):\n",
      "    \"\"\"\n",
      "    Convert list of label IDs to a list of one-hot encoding vectors\n",
      "    \"\"\"\n",
      "    n_labels = len(labels)\n",
      "    n_unique_labels = num_labels\n",
      "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
      "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
      "    return one_hot_encode\n",
      "\n",
      "## Helper functions for defining the network\n",
      "def weight_variable(shape):\n",
      "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
      "    return tf.Variable(initial)\n",
      "\n",
      "def bias_variable(shape):\n",
      "    initial = tf.constant(1.0, shape = shape)\n",
      "    return tf.Variable(initial)\n",
      "\n",
      "def conv2d(x, W):\n",
      "    return tf.nn.conv2d(x,W,strides=[1,2,2,1], padding='SAME')\n",
      "\n",
      "def apply_convolution(x,kernel_size,num_channels,depth):\n",
      "    weights = weight_variable([kernel_size, kernel_size, num_channels, depth])\n",
      "    biases = bias_variable([depth])\n",
      "    return tf.nn.relu(tf.add(conv2d(x, weights),biases))\n",
      "\n",
      "def apply_max_pool(x,kernel_size,stride_size):\n",
      "    return tf.nn.max_pool(x, ksize=[1, kernel_size, kernel_size, 1], \n",
      "                          strides=[1, stride_size, stride_size, 1], padding='SAME')\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Loading the test and train clips.\n",
      "with open(FILENAMES,\"r\") as fh:\n",
      "    filecontents=fh.read()\n",
      "    filenames=filecontents.split('\\n')\n",
      "    filenames=filenames[:-1] \n",
      "    filenames = [DATASET_LOCATION+f for f in filenames]\n",
      "random.shuffle(filenames)\n",
      "filenames = filenames[:2250]\n",
      "rnd_indices = np.random.rand(len(filenames)) < 0.70\n",
      "\n",
      "print len(rnd_indices)\n",
      "train = []\n",
      "test = []\n",
      "for i in range(len(filenames)):\n",
      "    if rnd_indices[i]:\n",
      "        train.append(rnd_indices)\n",
      "    else:\n",
      "        test.append(rnd_indices)\n",
      "print \"Train examples: \", len(train)\n",
      "print \"Test examples: \", len(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Defining the network as a TensorFlow computational graph\n",
      "X = tf.placeholder(tf.float32, shape=[None,bands,frames,num_channels])\n",
      "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
      "cov = apply_convolution(X,kernel_size,num_channels,depth)\n",
      "shape = cov.get_shape().as_list()\n",
      "cov_flat = tf.reshape(cov, [-1, shape[1] * shape[2] * shape[3]])\n",
      "f_weights = weight_variable([shape[1] * shape[2] * depth, num_hidden])\n",
      "f_biases = bias_variable([num_hidden])\n",
      "f = tf.nn.sigmoid(tf.add(tf.matmul(cov_flat, f_weights),f_biases))\n",
      "out_weights = weight_variable([num_hidden, num_labels])\n",
      "out_biases = bias_variable([num_labels])\n",
      "pred = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Defining the loss function \n",
      "cross_entropy = -tf.reduce_sum(Y * tf.log(pred))\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
      "#train_prediction = tf.nn.softmax(cross_entropy)\n",
      "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(Y,1))\n",
      "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Running the computational graph\n",
      "# We run the training algorithm in batches and compute the loss \n",
      "# for each batch, and optimize the network weights accordingly.\n",
      "# In the end, we look at the accuracy of the trained network on the\n",
      "# test set. \n",
      "cost_history = np.empty(shape=[1],dtype=float)\n",
      "with tf.Session() as session:\n",
      "    tf.initialize_all_variables().run()\n",
      "    for itr in range(training_iterations):    \n",
      "        offset = (itr * batch_size) % (len(train) - batch_size)\n",
      "        print offset\n",
      "        batch = filenames[offset:(offset + batch_size)]\n",
      "        batch_x, batch_y = extract_features(batch)\n",
      "        batch_y = one_hot_encode(batch_y)\n",
      "        print batch_y.shape, batch_x.shape\n",
      "        _, c = session.run([optimizer, cross_entropy],feed_dict={X: batch_x, Y : batch_y})\n",
      "        cost_history = np.append(cost_history,c)\n",
      "    test_x, test_y = extract_features(test)\n",
      "    print('Test accuracy: ',round(session.run(accuracy, feed_dict={X: test_x, Y: test_y}) , 3))\n",
      "    fig = plt.figure(figsize=(15,10))\n",
      "    plt.plot(cost_history)\n",
      "    plt.axis([0,training_iterations,0,np.max(cost_history)])\n",
      "    plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}